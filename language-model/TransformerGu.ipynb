{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e318df61",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f807bf9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-06 17:44:23.634512: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-04-06 17:44:23.634542: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "from fastai.text import *\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "import sentencepiece as spm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4ea0137",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('1.0.57', '1.11.0')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import fastai, torch\n",
    "fastai.__version__ , torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6edc6c75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a47d1224",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path('/home/jazzyy/Text_Summ/language-model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2499a7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from inltk.tokenizer import GujaratiTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf9326b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = spm.SentencePieceProcessor()\n",
    "sp.Load(str(path/\"../Tokenization/gujarati_lm.model\"))\n",
    "itos = [sp.IdToPiece(int(i)) for i in range(20000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19436048",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<unk>',\n",
       " '<s>',\n",
       " '</s>',\n",
       " '▁',\n",
       " '.',\n",
       " ',',\n",
       " '▁છે',\n",
       " '▁અને',\n",
       " 'ની',\n",
       " 'ના',\n",
       " 'માં',\n",
       " '▁આ',\n",
       " 'ને',\n",
       " '▁જ',\n",
       " '▁એક',\n",
       " 'નો',\n",
       " '▁આવેલા',\n",
       " '▁તેમ',\n",
       " 'નું',\n",
       " '▁કે',\n",
       " '▁ગામમાં',\n",
       " 'થી',\n",
       " '▁માટે',\n",
       " '▁પણ',\n",
       " '▁ખેતી',\n",
       " '▁આવે',\n",
       " '▁કરવામાં',\n",
       " '▁તે',\n",
       " '▁ભાગમાં',\n",
       " '-',\n",
       " 'ો',\n",
       " 'ે',\n",
       " '▁હતી',\n",
       " '�.',\n",
       " '▁પર',\n",
       " 'ી',\n",
       " '▁જે',\n",
       " '▁હતા',\n",
       " '▁ગામ',\n",
       " '▁ભારત',\n",
       " '▁મુખ્ય',\n",
       " 'એ',\n",
       " '▁હતો',\n",
       " 'X',\n",
       " '▁આવેલું',\n",
       " '▁ગુજરાત',\n",
       " '▁હતું',\n",
       " '▁એ',\n",
       " '▁સાથે',\n",
       " '▁કરી',\n",
       " '▁પશ્ચિમ',\n",
       " '▁રાજ્યના',\n",
       " '▁દેશના',\n",
       " '▁ડેરી',\n",
       " '▁શાળા',\n",
       " '▁મુખ્યત્વે',\n",
       " '▁અન્ય',\n",
       " '▁જેવી',\n",
       " '▁પ્રાથમિક',\n",
       " 'ં',\n",
       " 'દૂધની',\n",
       " '▁તરીકે',\n",
       " '▁હોય',\n",
       " '▁ગામના',\n",
       " '▁પશુપાલન',\n",
       " '▁દ્વારા',\n",
       " '▁ખેતમજૂરી',\n",
       " '▁આંગણવાડી',\n",
       " '▁વ્યવસાય',\n",
       " '▁પંચાયતઘર',\n",
       " '▁પ્રાપ્ય',\n",
       " '▁સવલતો',\n",
       " '▁કરે',\n",
       " '▁લોકોનો',\n",
       " '▁તાલુકામાં',\n",
       " '▁જિલ્લામાં',\n",
       " '▁પાક',\n",
       " '▁થયેલી',\n",
       " 'ા',\n",
       " 'સ',\n",
       " '▁થાય',\n",
       " '�',\n",
       " '▁શાકભાજીના',\n",
       " '▁કુલ',\n",
       " 'ર',\n",
       " '▁તેઓ',\n",
       " '▁એવા',\n",
       " 'ક',\n",
       " 'ન',\n",
       " '▁���',\n",
       " '▁રીતે',\n",
       " '▁��',\n",
       " '▁બાજરી',\n",
       " '▁\"',\n",
       " 'લ',\n",
       " '▁લોકો',\n",
       " '▁પૈકીના',\n",
       " '▁તેમણે',\n",
       " '▁કપાસ',\n",
       " '▁તેના',\n",
       " '\"',\n",
       " '▁તાલુકાઓ',\n",
       " 'નાં',\n",
       " 'ઓ',\n",
       " '▁આવી',\n",
       " '▁વધુ',\n",
       " '▁તેમના',\n",
       " '▁અથવા',\n",
       " '▁ઘઉં',\n",
       " 'તા',\n",
       " '▁સુધી',\n",
       " ':',\n",
       " '▁સૌથી',\n",
       " '▁બે',\n",
       " '▁તેને',\n",
       " '▁ઉપયોગ',\n",
       " '▁નથી',\n",
       " '▁કરવા',\n",
       " 'મ',\n",
       " '▁તેની',\n",
       " '▁કારણે',\n",
       " 'માંથી',\n",
       " '▁પછી',\n",
       " '▁શકે',\n",
       " '▁ઉત્તર',\n",
       " '્સ',\n",
       " '▁ન',\n",
       " '▁કર્યો',\n",
       " '▁જેમાં',\n",
       " '▁તથા',\n",
       " '▁રજકો',\n",
       " '▁જીરુ',\n",
       " '▁જ્યારે',\n",
       " \"'\",\n",
       " '▁કર્યું',\n",
       " 's',\n",
       " '▁ના',\n",
       " '▁પરંતુ',\n",
       " '▁પ્રથમ',\n",
       " '▁તો',\n",
       " '▁દક્ષિણ',\n",
       " '▁નામ',\n",
       " '▁મધ્ય',\n",
       " '▁તેમની',\n",
       " '▁બાદ',\n",
       " '▁કરીને',\n",
       " '▁આવ્યું',\n",
       " '▁ખાતે',\n",
       " '▁અ',\n",
       " '▁ભારતીય',\n",
       " '▁થયો',\n",
       " '▁દિવેલા',\n",
       " '▁થઈ',\n",
       " '▁સમાવેશ',\n",
       " 'ત',\n",
       " '▁તેમને',\n",
       " '▁વર્ષ',\n",
       " 'મી',\n",
       " '▁આવ્યો',\n",
       " '▁વિસ્તારમાં',\n",
       " '▁કરતા',\n",
       " '▁ત્યારે',\n",
       " '▁તેમજ',\n",
       " '▁દરમિયાન',\n",
       " 'ડ',\n",
       " '▁કેટલાક',\n",
       " '▁રોજ',\n",
       " '▁સામાન્ય',\n",
       " 'જ',\n",
       " 'મા',\n",
       " '▁માં',\n",
       " '▁મોટા',\n",
       " '▁આવ્યા',\n",
       " '▁ભાગ',\n",
       " \"▁'\",\n",
       " '▁ઉપરાંત',\n",
       " '▁અહીં',\n",
       " '▁શકાય',\n",
       " '▁‘',\n",
       " '▁the',\n",
       " 'લી',\n",
       " 'લા',\n",
       " '▁ધરાવે',\n",
       " 'િત',\n",
       " '▁ઓફ',\n",
       " '▁મંદિર',\n",
       " '▁જિલ્લાના',\n",
       " '▁જેવા',\n",
       " '▁સૌરાષ્ટ્ર',\n",
       " '▁ડાંગર',\n",
       " 'આ',\n",
       " '▁એમ',\n",
       " 'જી',\n",
       " ';',\n",
       " '▁ખાસ',\n",
       " '▁પૂર્વ',\n",
       " 'ટ',\n",
       " 'ડી',\n",
       " '▁જોવા',\n",
       " '▁જેમ',\n",
       " '▁પોતાના',\n",
       " '▁રાજ્ય',\n",
       " '▁મુજબ',\n",
       " 'પ',\n",
       " 'ય',\n",
       " 'રા',\n",
       " 'વા',\n",
       " '▁દિવસ',\n",
       " '▁જો',\n",
       " 'વ',\n",
       " '’',\n",
       " '▁મળે',\n",
       " '▁રહે',\n",
       " '▁વગેરે',\n",
       " '▁વચ્ચે',\n",
       " 'ઝ',\n",
       " 'રી',\n",
       " '▁વિવિધ',\n",
       " '▁કામ',\n",
       " '▁સમય',\n",
       " '▁થયા',\n",
       " '▁મગફળી',\n",
       " '▁તેમાં',\n",
       " 'ડા',\n",
       " '▁તલ',\n",
       " '▁કોઈ',\n",
       " '▁ધ',\n",
       " '▁થયું',\n",
       " '▁થઇ',\n",
       " '▁ચણા',\n",
       " '▁વસ્તી',\n",
       " 'ળ',\n",
       " '▁જાય',\n",
       " '▁ગુજરાતી',\n",
       " '/',\n",
       " '▁અલગ',\n",
       " '▁મકાઈ',\n",
       " '▁ત્રણ',\n",
       " '▁શહેર',\n",
       " '▁ઘણા',\n",
       " '▁તાલુકા',\n",
       " '▁શરૂ',\n",
       " '▁આવેલ',\n",
       " 'ણ',\n",
       " '▁સામે',\n",
       " 'વી',\n",
       " 'બ',\n",
       " 'શ',\n",
       " '▁બીજા',\n",
       " '▁ધરાવતા',\n",
       " 'ગ',\n",
       " '▁of',\n",
       " '▁ત્યાં',\n",
       " '▁કરવાની',\n",
       " '▁ઇ',\n",
       " 'કા',\n",
       " '▁દિવેલી',\n",
       " '▁તેનો',\n",
       " '▁બી',\n",
       " '▁પોતાની',\n",
       " '▁ફિલ્મ',\n",
       " '▁સ',\n",
       " '▁બની',\n",
       " '▁ઉપર',\n",
       " '▁કર્યા',\n",
       " '▁થી',\n",
       " '▁ઈ',\n",
       " 'ઇ',\n",
       " '▁આપવામાં',\n",
       " '▁ઘણી',\n",
       " '▁ભારતના',\n",
       " '▁જન્મ',\n",
       " 'સી',\n",
       " 'િક',\n",
       " '▁મે',\n",
       " '▁૬',\n",
       " '▁નદી',\n",
       " '▁વધારે',\n",
       " '▁તરફ',\n",
       " '▁કરતાં',\n",
       " 'ું',\n",
       " '▁તુવર',\n",
       " 'ુ',\n",
       " 'તી',\n",
       " '▁ગયા',\n",
       " '▁છતાં',\n",
       " '▁માત્ર',\n",
       " 'દ',\n",
       " '▁હેઠળ',\n",
       " '▁હતાં',\n",
       " '▁આવેલી',\n",
       " '▁કારણ',\n",
       " '▁આદિવાસી',\n",
       " 'ીય',\n",
       " '▁દર',\n",
       " '▁તેણે',\n",
       " '▁in',\n",
       " '▁તેનું',\n",
       " 'કો',\n",
       " '▁સમયે',\n",
       " '▁લગભગ',\n",
       " '▁મળી',\n",
       " 'ઈ',\n",
       " '▁ટકા',\n",
       " 'પુર',\n",
       " '▁તમામ',\n",
       " '▁દરેક',\n",
       " '%',\n",
       " '▁શાકભાજી',\n",
       " '▁વખત',\n",
       " '▁ભાષા',\n",
       " 'હ',\n",
       " '▁જ્યાં',\n",
       " '▁મેળવી',\n",
       " '▁આપી',\n",
       " '▁વિસ્તાર',\n",
       " 'િ',\n",
       " '▁અભ્યાસ',\n",
       " '▁રહી',\n",
       " 'િંગ',\n",
       " '▁રહ્યા',\n",
       " '▁મૂળ',\n",
       " '▁નજીક',\n",
       " '▁જેને',\n",
       " '▁જોકે',\n",
       " 'ોમાં',\n",
       " '▁પાસે',\n",
       " '▁કોઇ',\n",
       " '▁ખૂબ',\n",
       " '▁૧૦',\n",
       " '▁રાષ્ટ્રીય',\n",
       " '▁જાહેર',\n",
       " '▁રચના',\n",
       " '▁હોવા',\n",
       " '▁ચાર',\n",
       " '▁પ્રમાણે',\n",
       " '▁“',\n",
       " '▁and',\n",
       " 'ંગ',\n",
       " '▁બહાર',\n",
       " 'રો',\n",
       " '▁સેવા',\n",
       " '▁અનેક',\n",
       " 'મો',\n",
       " '▁સ્થાપના',\n",
       " '▁શબ્દ',\n",
       " '્યા',\n",
       " '▁યુદ્ધ',\n",
       " '▁બંને',\n",
       " '▁દૂર',\n",
       " '▁આપે',\n",
       " '▁પાંચ',\n",
       " '▁મોટી',\n",
       " 'ફ',\n",
       " 'ચ',\n",
       " '્',\n",
       " '▁પહેલા',\n",
       " '▁વરિયાળી',\n",
       " '•',\n",
       " 'યા',\n",
       " '▁મૃત્યુ',\n",
       " 'અ',\n",
       " 'િયા',\n",
       " '▁વિ',\n",
       " '▁યુ',\n",
       " 'ટી',\n",
       " '▁એટલે',\n",
       " '▁પરથી',\n",
       " '▁અનુસાર',\n",
       " '▁મથક',\n",
       " '2',\n",
       " 'સા',\n",
       " '▁નવા',\n",
       " 'એસ',\n",
       " '▁વિકાસ',\n",
       " '▁સરકાર',\n",
       " 'ોને',\n",
       " '▁દિવસે',\n",
       " '▁૧૧',\n",
       " '▁અંગે',\n",
       " '▁કંપની',\n",
       " '▁રાજા',\n",
       " '▁સ્થાન',\n",
       " 'a',\n",
       " '્યો',\n",
       " '▁ધર્મ',\n",
       " 'કે',\n",
       " '▁નીચે',\n",
       " '▁કાર્ય',\n",
       " '▁પ્રદેશ',\n",
       " '▁એવી',\n",
       " 'બી',\n",
       " '▁સ્થળ',\n",
       " 'ed',\n",
       " '▁શિક્ષણ',\n",
       " '▁આમ',\n",
       " '▁બંધ',\n",
       " '▁લે',\n",
       " '▁અસર',\n",
       " '▁વિશ્વ',\n",
       " '▁૯',\n",
       " '▁જીવન',\n",
       " '▁માર્ગ',\n",
       " '▁બનાવવામાં',\n",
       " '▁પાણી',\n",
       " '▁દેશ',\n",
       " 'વે',\n",
       " '▁કહેવાય',\n",
       " 'તો',\n",
       " '▁કેન્દ્ર',\n",
       " '▁નહીં',\n",
       " '▁તમાકુ',\n",
       " '▁હોવાથી',\n",
       " '▁શરૂઆત',\n",
       " 'પુરા',\n",
       " '▁શેરડી',\n",
       " '▁જેવાં',\n",
       " '▁સુ',\n",
       " '▁પડે',\n",
       " '▁ઉત્પાદન',\n",
       " 'રે',\n",
       " '▁a',\n",
       " '▁એવું',\n",
       " '▁સમગ્ર',\n",
       " '▁નાના',\n",
       " 'થ',\n",
       " '”',\n",
       " 'પી',\n",
       " '▁ભારતમાં',\n",
       " '▁કહે',\n",
       " '▁પ્રાપ્ત',\n",
       " '▁ઉપલબ્ધ',\n",
       " '▁આધારિત',\n",
       " '▁લેવામાં',\n",
       " '▁દિશામાં',\n",
       " '▁મદદ',\n",
       " 'પર',\n",
       " '▁કેટલીક',\n",
       " '▁આગળ',\n",
       " 'લે',\n",
       " '▁૧',\n",
       " '▁કા',\n",
       " '▁૧૪',\n",
       " '▁to',\n",
       " '▁રાજ',\n",
       " '▁વ્યક્તિ',\n",
       " '▁જિલ્લો',\n",
       " '▁18',\n",
       " '▁શ્રી',\n",
       " '▁અર્થ',\n",
       " '▁સુરત',\n",
       " 'ધ',\n",
       " '▁છ',\n",
       " '▁નિર્માણ',\n",
       " '▁૫',\n",
       " 'બા',\n",
       " '▁પ્રાચીન',\n",
       " '▁૮',\n",
       " '▁યુનાઇટેડ',\n",
       " '▁અમુક',\n",
       " '▁પ્રકારના',\n",
       " '▁મા',\n",
       " '▁જમીન',\n",
       " '▁લેખ',\n",
       " '▁માહિતી',\n",
       " 'ીયા',\n",
       " '▁વસે',\n",
       " 'ખ',\n",
       " '▁સમયમાં',\n",
       " 'વિ',\n",
       " '▁આવતા',\n",
       " '▁કો',\n",
       " '▁સ્થાનિક',\n",
       " '▁પાન',\n",
       " '▁સાહિત્ય',\n",
       " '▁ભગવાન',\n",
       " 'ન્ટ',\n",
       " 'લો',\n",
       " '▁રજૂ',\n",
       " '▁મ',\n",
       " 'નુ',\n",
       " '▁પહેલાં',\n",
       " '▁બીજી',\n",
       " 'દા',\n",
       " 'સ્',\n",
       " '▁બન્યા',\n",
       " '▁હવે',\n",
       " 'જે',\n",
       " '▁કેળાં',\n",
       " '▁બ',\n",
       " '▁ક્ષેત્ર',\n",
       " '▁પ્રમાણમાં',\n",
       " '▁ત્યાર',\n",
       " '▁સમાન',\n",
       " '▁હિંદુ',\n",
       " '▁તેવી',\n",
       " '▁એન્ડ',\n",
       " '▁અમદાવાદ',\n",
       " '▁તેવા',\n",
       " '▁કિ',\n",
       " 'ોની',\n",
       " '▁નગર',\n",
       " '▁પ્ર',\n",
       " '▁રંગ',\n",
       " '▁સે',\n",
       " '▁તેથી',\n",
       " '▁અહીંના',\n",
       " '▁લોકોની',\n",
       " '્ય',\n",
       " '▁આધુનિક',\n",
       " '▁ઓછા',\n",
       " '▁વડે',\n",
       " '▁આશરે',\n",
       " '▁તેમનું',\n",
       " '▁બનાવવા',\n",
       " '▁પૈકી',\n",
       " 'ડો',\n",
       " '▁મગ',\n",
       " 'ાઈ',\n",
       " '▁જુવાર',\n",
       " '▁પ્રક્રિયા',\n",
       " '▁નવી',\n",
       " 'ીંગ',\n",
       " 'e',\n",
       " '▁પુત્ર',\n",
       " '▁તેમનો',\n",
       " '▁વ',\n",
       " '▁મહત્વ',\n",
       " '▁તૈયાર',\n",
       " '▁૧૮',\n",
       " '▁ખેત',\n",
       " 'd',\n",
       " '▁જેના',\n",
       " '▁જરૂર',\n",
       " '▁વધારો',\n",
       " '▁રાજ્યમાં',\n",
       " '▁હાથ',\n",
       " '▁બ્રિટિશ',\n",
       " '▁લગ્ન',\n",
       " '▁યોગ્ય',\n",
       " '▁આપ્યો',\n",
       " '▁સગવડ',\n",
       " 'જા',\n",
       " '▁અંગ્રેજી',\n",
       " '▁-',\n",
       " '▁કચ્છ',\n",
       " '▁અત્યંત',\n",
       " '▁1',\n",
       " '▁સંગીત',\n",
       " '▁માનવ',\n",
       " '▁થતો',\n",
       " '▁શાસન',\n",
       " '▁કાર્યો',\n",
       " '▁માનવામાં',\n",
       " '▁પોતાનું',\n",
       " 'ટા',\n",
       " '▁નોકરી',\n",
       " '1',\n",
       " '▁પ્રદેશમાં',\n",
       " '▁ચોક્કસ',\n",
       " 'વો',\n",
       " '▁દેવ',\n",
       " '▁અંત',\n",
       " '▁ચાલુ',\n",
       " '▁જિલ્લાનું',\n",
       " 'સે',\n",
       " 'કી',\n",
       " 'તે',\n",
       " '▁વખતે',\n",
       " '▁શોધ',\n",
       " '▁આંતરરાષ્ટ્રીય',\n",
       " '▁સામાજિક',\n",
       " '▁૭',\n",
       " '▁ગયો',\n",
       " '▁ત્યારબાદ',\n",
       " '▁આપ્યું',\n",
       " '▁સુધારો',\n",
       " '▁વડોદરા',\n",
       " '▁જંગલ',\n",
       " '▁પ',\n",
       " '▁પ્રકારની',\n",
       " '▁પદ્ધતિ',\n",
       " 'કર',\n",
       " '▁શક્કરીયાં',\n",
       " '▁ઓળખાય',\n",
       " 'ing',\n",
       " '▁ઉલ્લેખ',\n",
       " 'વામાં',\n",
       " '▁વિશાળ',\n",
       " '▁પી',\n",
       " '▁રહ્યો',\n",
       " 'વાળા',\n",
       " '▁ઓ',\n",
       " 'સર',\n",
       " '▁સંખ્યા',\n",
       " '▁લોકપ્રિય',\n",
       " '▁પિતા',\n",
       " '▁ભૂમિકા',\n",
       " '▁દરમ્યાન',\n",
       " '▁આવેલો',\n",
       " '▁ઓછી',\n",
       " '▁સી',\n",
       " 'y',\n",
       " 'ર્સ',\n",
       " '5',\n",
       " '▁બટાટા',\n",
       " '▁મુંબઈ',\n",
       " '▁વાત',\n",
       " 'C',\n",
       " '▁પૂરી',\n",
       " '▁કર',\n",
       " '્યુ',\n",
       " '▁પ્રકાશિત',\n",
       " '▁રાજકીય',\n",
       " '▁રહ્યું',\n",
       " '▁માતા',\n",
       " 'કાર',\n",
       " '▁જેથી',\n",
       " '▁૩',\n",
       " 'નિ',\n",
       " '▁ફરી',\n",
       " '▁બધા',\n",
       " '▁ઉચ્ચ',\n",
       " 'વર',\n",
       " '▁૪',\n",
       " 'ોએ',\n",
       " '▁બને',\n",
       " '▁હોવાનું',\n",
       " '▁પૂર્ણ',\n",
       " '▁જાણીતા',\n",
       " 'M',\n",
       " '▁શહેરમાં',\n",
       " 'ોના',\n",
       " '▁રામ',\n",
       " '▁બનાસકાંઠા',\n",
       " 'ન્સ',\n",
       " '▁પુસ્તક',\n",
       " '▁ઘણાં',\n",
       " '્યું',\n",
       " '▁કરતી',\n",
       " '▁૨',\n",
       " '▁દિલ્હી',\n",
       " '▁મહારાષ્ટ્ર',\n",
       " 'વાદ',\n",
       " '▁ડુંગરા',\n",
       " '▁વસવાટ',\n",
       " '▁પુરસ્કાર',\n",
       " '▁ડી',\n",
       " '▁ઉદાહરણ',\n",
       " 'ણા',\n",
       " '▁સત્તા',\n",
       " '▁વર્ષની',\n",
       " 'o',\n",
       " '▁અમેરિકન',\n",
       " '▁સ્થિત',\n",
       " '▁થયેલ',\n",
       " '▁સુધીમાં',\n",
       " '▁ઘર',\n",
       " '▁ભાવનગર',\n",
       " '▁વ્યવસ્થા',\n",
       " '▁ચા',\n",
       " '▁આર્થિક',\n",
       " '▁વિશે',\n",
       " '▁આદિવાસીઓ',\n",
       " 'યુ',\n",
       " '▁રોગ',\n",
       " '▁નામના',\n",
       " '▁વાર',\n",
       " 'p',\n",
       " '▁થતા',\n",
       " 'વું',\n",
       " '▁જૈન',\n",
       " '▁10',\n",
       " '▁એવો',\n",
       " 'ઉ',\n",
       " '▁વર્ષે',\n",
       " '▁ધાર્મિક',\n",
       " '▁પાડે',\n",
       " '▁બિન',\n",
       " '▁કરવાનો',\n",
       " '▁નક્કી',\n",
       " 'માન',\n",
       " '▁થતી',\n",
       " 'c',\n",
       " '▁2008',\n",
       " '4',\n",
       " '▁થયેલા',\n",
       " '▁ઓળખવા',\n",
       " '▁હાલમાં',\n",
       " '▁2',\n",
       " '▁મોટો',\n",
       " '▁પસાર',\n",
       " '’,',\n",
       " '▁કલા',\n",
       " '▁સમાજ',\n",
       " '▁વ્યવસાયમાં',\n",
       " '▁રહેલા',\n",
       " '▁બહુ',\n",
       " '▁જાન્યુઆરી',\n",
       " 'ખા',\n",
       " '▁ધ્યાન',\n",
       " '▁સિ',\n",
       " '▁ધરાવતી',\n",
       " '▁નિ',\n",
       " '▁2007',\n",
       " '▁વર્ષના',\n",
       " '▁સંશોધન',\n",
       " 'ણી',\n",
       " 'મે',\n",
       " '▁યુનિવર્સિટી',\n",
       " 'ઓને',\n",
       " 'લિ',\n",
       " '▁કહ્યું',\n",
       " '▁આવતી',\n",
       " '▁પડી',\n",
       " '▁ઉ',\n",
       " '▁સિવાય',\n",
       " '▁લાંબા',\n",
       " '▁ગો',\n",
       " '▁લીધે',\n",
       " '▁રે',\n",
       " '▁નોંધપાત્ર',\n",
       " '▁લેવા',\n",
       " '▁અંદર',\n",
       " 'n',\n",
       " '▁પ્રમુખ',\n",
       " '▁સંપૂર્ણ',\n",
       " '▁સાત',\n",
       " 'ગા',\n",
       " '▁મોટું',\n",
       " '▁લઈ',\n",
       " '▁બોલી',\n",
       " '▁યોજના',\n",
       " 'ભ',\n",
       " '▁હે',\n",
       " '▁આપવા',\n",
       " '▁15',\n",
       " '▁સ્વરૂપ',\n",
       " '°',\n",
       " 'યો',\n",
       " '▁આજે',\n",
       " 'ઘ',\n",
       " 'ડે',\n",
       " '▁જણાવ્યું',\n",
       " '▁વે',\n",
       " 'ઠ',\n",
       " 'તિ',\n",
       " '▁રાજકોટ',\n",
       " '▁એપ્રિલ',\n",
       " '▁પ્રખ્યાત',\n",
       " '▁જેટલા',\n",
       " '▁20',\n",
       " '▁ભાગના',\n",
       " '▁સદી',\n",
       " 'શે',\n",
       " '▁ક્ષમતા',\n",
       " '▁કી',\n",
       " 't',\n",
       " '▁પ્રમાણ',\n",
       " '▁ઇતિહાસ',\n",
       " '▁નેટવર્ક',\n",
       " '▁સતત',\n",
       " '▁આસપાસ',\n",
       " '▁લીધો',\n",
       " '▁જેઓ',\n",
       " '▁સંગ્રહ',\n",
       " '▁સ્તર',\n",
       " '▁જેટલી',\n",
       " '▁પ્રવેશ',\n",
       " '▁16',\n",
       " '▁રાખવામાં',\n",
       " '▁નામે',\n",
       " '▁સભ્ય',\n",
       " '▁સૂર્ય',\n",
       " '▁સૈનિકો',\n",
       " '▁17',\n",
       " '▁is',\n",
       " '▁વિચાર',\n",
       " '▁જરૂરી',\n",
       " 'દાર',\n",
       " '▁બાળકો',\n",
       " 'વાડા',\n",
       " '▁3',\n",
       " '▁ભાગે',\n",
       " 'ીઓ',\n",
       " '▁ઉપ',\n",
       " 'er',\n",
       " '▁સ્ટેટ્સ',\n",
       " 'લાલ',\n",
       " '▁ક્રિકેટ',\n",
       " '▁સ્ટેશન',\n",
       " 'al',\n",
       " '▁સપાટી',\n",
       " ')',\n",
       " '▁થોડા',\n",
       " '▁ગયું',\n",
       " '▁સંસ્કૃતિ',\n",
       " '▁પૃથ્વી',\n",
       " 'સ્ટ',\n",
       " '▁સો',\n",
       " '▁બનાવી',\n",
       " 'ઠી',\n",
       " '▁કહેવામાં',\n",
       " '▁પ્રસિદ્ધ',\n",
       " 'ર્',\n",
       " '.\"',\n",
       " '▁શ્રેષ્ઠ',\n",
       " '▁ફક્ત',\n",
       " 'ેલા',\n",
       " '▁વિભાગ',\n",
       " '▁જેનો',\n",
       " 'ૂ',\n",
       " '▁કરવાનું',\n",
       " '▁સંસ્થા',\n",
       " '▁સં',\n",
       " '▁સ્',\n",
       " '▁સદીના',\n",
       " '▁સફળ',\n",
       " 'ાઇ',\n",
       " 'તર',\n",
       " 'ીક',\n",
       " '▁સ્વતંત્રતા',\n",
       " '▁કવિ',\n",
       " '▁ઐતિહાસિક',\n",
       " '▁સંયુક્ત',\n",
       " '▁પિયત',\n",
       " '▁દેશોમાં',\n",
       " '▁લોકોને',\n",
       " '▁આધારે',\n",
       " 'બે',\n",
       " 'ઓમાં',\n",
       " '▁આયોજન',\n",
       " '▁રેલ્વે',\n",
       " '▁at',\n",
       " '▁૧૭',\n",
       " '▁આધાર',\n",
       " '▁સપ્ટેમ્બર',\n",
       " 'ગો',\n",
       " '▁ત',\n",
       " '▁દા',\n",
       " '▁લીધી',\n",
       " '▁ક',\n",
       " '▁સર',\n",
       " '▁ગાંધી',\n",
       " '▁પ્રેમ',\n",
       " '▁કામગીરી',\n",
       " '▁જાહેરાત',\n",
       " '▁પા',\n",
       " '▁હ',\n",
       " '▁મહા',\n",
       " '▁દે',\n",
       " '▁વી',\n",
       " 'મિ',\n",
       " '▁ડિસેમ્બર',\n",
       " '▁સામેલ',\n",
       " '▁જ્ઞાન',\n",
       " '▁ડિ',\n",
       " '▁રસ',\n",
       " '▁થવા',\n",
       " '▁પ્રતિ',\n",
       " '▁મીટર',\n",
       " '▁રમત',\n",
       " '▁મુ',\n",
       " '▁નવેમ્બર',\n",
       " 'તું',\n",
       " 'b',\n",
       " '▁માર્ચ',\n",
       " '▁વર્ષો',\n",
       " '▁રાજધાની',\n",
       " '▁સ્થિતિ',\n",
       " '▁જૂન',\n",
       " '▁લા',\n",
       " '▁ડે',\n",
       " '▁કમ્પ્યુટર',\n",
       " '▁અરવલ્લી',\n",
       " '▁૧૫',\n",
       " '▁નાની',\n",
       " '▁દર્શાવે',\n",
       " '▁કાયદા',\n",
       " 'સો',\n",
       " '▁not',\n",
       " 'ગઢ',\n",
       " '▁પરિણામે',\n",
       " '▁ટીમ',\n",
       " '▁પરંપરાગત',\n",
       " '▁80',\n",
       " '▁વર્ણન',\n",
       " '▁સંચાલન',\n",
       " 'રુ',\n",
       " '▁ન્યૂ',\n",
       " '▁અગાઉ',\n",
       " '▁ભારે',\n",
       " 'ેલી',\n",
       " '▁2009',\n",
       " '▁જોઈએ',\n",
       " '▁લોક',\n",
       " '▁નવ',\n",
       " '3',\n",
       " '▁સમુદ્ર',\n",
       " '▁કાઢી',\n",
       " '▁સંબંધ',\n",
       " '▁સદીમાં',\n",
       " '▁મહ',\n",
       " '0',\n",
       " '▁કોલેજ',\n",
       " '▁લાખ',\n",
       " '▁પાછળ',\n",
       " '▁જતા',\n",
       " 'રામ',\n",
       " '▁તળાવ',\n",
       " '▁છોટાઉદેપુર',\n",
       " 'ન્',\n",
       " 'l',\n",
       " 'f',\n",
       " 'u',\n",
       " 'કુ',\n",
       " '▁વિજય',\n",
       " '▁રિ',\n",
       " '▁સહિત',\n",
       " 'સં',\n",
       " '▁કૃષ્ણ',\n",
       " '▁વર્લ્ડ',\n",
       " '▁ફેબ્રુઆરી',\n",
       " '▁વસતી',\n",
       " '▁આવા',\n",
       " '6',\n",
       " 'પા',\n",
       " '▁બન્યું',\n",
       " '▁બદલે',\n",
       " '▁પાસેથી',\n",
       " '▁મળ્યો',\n",
       " '▁સરેરાશ',\n",
       " '▁મેળવવા',\n",
       " '▁જળ',\n",
       " '▁પ્રભાવ',\n",
       " '▁માટેની',\n",
       " '▁જોઇએ',\n",
       " '્ડ',\n",
       " '▁વિશેષ',\n",
       " '▁સક્રિય',\n",
       " 'દેવ',\n",
       " 'N',\n",
       " 'એમ',\n",
       " 'ક્સ',\n",
       " '▁સંસ્થાઓ',\n",
       " 'ઃ',\n",
       " '▁ભારતની',\n",
       " 'ાં',\n",
       " '▁ધી',\n",
       " '▁શિવ',\n",
       " '▁ડોલર',\n",
       " '▁4',\n",
       " '▁સૈન્ય',\n",
       " 'શી',\n",
       " '▁દેશો',\n",
       " '▁મિ',\n",
       " 'જો',\n",
       " '▁લાલ',\n",
       " 'રાજ',\n",
       " '▁ગણવા',\n",
       " '▁ખર્ચ',\n",
       " '▁લશ્કરી',\n",
       " '▁સારી',\n",
       " 'દી',\n",
       " '▁પાટણ',\n",
       " '▁લેખક',\n",
       " '▁રીત',\n",
       " 'ીત',\n",
       " '▁ગઈ',\n",
       " '▁કાર',\n",
       " 'સુ',\n",
       " '▁રૂપે',\n",
       " '▁કુ',\n",
       " '▁વિરોધ',\n",
       " 'શા',\n",
       " 'i',\n",
       " '▁પસંદગી',\n",
       " '▁પે',\n",
       " '▁ભ',\n",
       " '▁વેચાણ',\n",
       " '▁કથા',\n",
       " '▁બ્રિટીશ',\n",
       " 'મ્',\n",
       " '▁પસંદ',\n",
       " '▁૧૨',\n",
       " '▁શ્રેણી',\n",
       " 'ાત્મક',\n",
       " '▁ફરીથી',\n",
       " '૨',\n",
       " '▁અમેરિકા',\n",
       " '▁જેની',\n",
       " '▁શરીર',\n",
       " '▁જીવ',\n",
       " '૦૦૦',\n",
       " '▁અંતિમ',\n",
       " '▁મી',\n",
       " '▁ઉદ્યોગ',\n",
       " '▁નામની',\n",
       " '▁કિમી',\n",
       " 'ેશ્વર',\n",
       " ...]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "itos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15bfac8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gujarati_vocab = Vocab(itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0591199b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(tok_func=GujaratiTokenizer, lang='gu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8088d4ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jazzyy/anaconda3/lib/python3.9/site-packages/fastai/core.py:299: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return np.array(a, dtype=dtype, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_lm = TextLMDataBunch.from_folder(path=path, tokenizer=tokenizer, vocab=gujarati_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "431cc9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lm.batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c3d237d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>idx</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>▁છે . ▁આ ▁ગામમાં ▁પ્રાથમિક ▁શાળા , ▁પંચાયતઘર , ▁આંગણવાડી ▁તેમ ▁જ ▁દૂધ ની ▁ડેરી ▁જેવી ▁સવલતો ▁પ્રાપ્ય ▁થયેલી ▁છે . ▁x x bo s ▁ગુજરાત ▁અને ▁ભારતમાં ▁સ્થાન ▁કાન સ રીયા ▁એ ▁ભારત ▁દેશના ▁પશ્ચિમ ▁ભાગમાં ▁આવેલા ▁ગુજરાત ▁રાજ્યના ▁ગીર ▁સોમનાથ ▁જિલ્લાના ▁ગીર ▁ગઢડા ▁તાલુકામાં ▁આવેલું ▁ગામ ▁છે . ▁આ ▁ગામના ▁લોકોનો ▁મુખ્ય ▁વ્યવસાય ▁ખેતી , ▁ખેતમજૂરી ▁તેમ ▁જ ▁પશુપાલન ▁છે . ▁ગામમાં ▁મુખ્યત્વે ▁ઘઉં , ▁બાજરો , ▁કપાસ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>▁ડેસ િઆ નો ▁ભાગ ▁બની ▁ગયુ . ▁ખનીજ ની ▁સમૃદ્ધ ▁અનામત ો ▁પ્રદેશમાં થી ▁મળી ▁આવી ▁અને ▁ખાસ ▁કરીને ▁સોના ▁તથા ▁ચાંદી ની ▁માત્રા ▁અત્યંત ▁વધારે ▁હતી . ▁જેના ▁પરિણામે ▁રોમ ે ▁આ ▁પ્રાંત નું ▁ભારે ▁સંસ્થાન ીકરણ ▁કર્યુ . ▁આનાથી ▁ગ્રામ્ય ▁લેટિન ▁આવ્યું ▁અને ▁ગહન રો મન - કરણ નો ▁દોર ▁આવ્યો ▁કે ▁જેનાથી ▁મૂળ - રો માન િયન નો ▁સમય ▁શરૂ ▁થયો . ▁આ ▁ઉપરાંત ▁ઈસ . ▁3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>▁છે . ▁જેઠ ▁- વૈશાખ માં ▁તે ▁રોપ ાય ▁છે ▁અને ▁માગ સર - પોષ ▁માં ▁તે ▁ખોદી ▁કાઢવા માં ▁આવે ▁છે . ▁ભારતમાં ▁સર્વત્ર ▁થાય ▁છે . ▁સૂરણ માં ▁બે ▁જાત ▁હોય ▁છે : ▁એક ▁મીઠી ▁અને ▁બીજી ▁ખૂ જ લી વાળી , ખ ૂ જ લી વાળુ ▁સૂરણ ▁ખાવા થી ▁વ વળ ાટ ▁થાય ▁છે ▁અને ▁મોં ▁સૂ જી ▁જાય ▁છે . ▁આવા ▁સૂરણ નો ▁કંદ ▁લી સો</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>. ▁શહેરના ▁ઇતિહાસમાં ▁બે ▁વખત , ▁નામ ને ▁બેઇજિંગ ▁થી ▁બે ઇ પિંગ માં ▁બદલવા માં ▁આવેલું ; &lt;unk&gt; િક ▁અર્થે ▁' ઉત્તર ી ▁શાંતિ ' . ▁આ ▁પ્રથમ ▁વખત ▁મિંગ ▁સામ્રાજ્યના ▁હોંગ હુ ▁સમ્રાટ ▁હેઠળ ▁બન્યું , ▁અને ▁ફરીથી ▁1928 ▁માં ▁રીપબ્લિક ▁ઓફ ▁ચાઈના ના ▁ક્યુ ઓ મિ ન્ટે ંગ ▁સરકાર માં ▁બન્યું . ▁દરેક ▁પ્રસંગે , ▁બદલે લ ▁નામ ▁' રાજ ધા ની ' ▁ના ▁અર્થ ના ▁તત્ત્વ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>▁ભાગમાં ▁આવેલા ▁ગુજરાત ▁રાજ્યના ▁સૌરાષ્ટ્ર ▁વિસ્તારના ▁જૂનાગઢ ▁જિલ્લામાં ▁આવેલા ▁વિસાવદર ▁તાલુકામાં ▁આવેલું ▁એક ▁ગામ ▁છે . ▁આ ▁ગામના ▁લોકોનો ▁મુખ્ય ▁વ્યવસાય ▁ખેતી , ▁ખેતમજૂરી ▁તેમ ▁જ ▁પશુપાલન ▁છે . ▁આ ▁ગામમાં ▁મુખ્યત્વે ▁ઘઉં , ▁જીરુ , ▁મગફળી , ▁તલ , ▁બાજરી , ▁કપાસ , ▁દિવેલા , ▁રજકો ▁તેમ ▁જ ▁અન્ય ▁શાકભાજીના ▁પાક ની ▁ખેતી ▁કરવામાં ▁આવે ▁છે . ▁આ ▁ગામમાં ▁પ્રાથમિક ▁શાળા , ▁પંચાયતઘર , ▁આંગણવાડી ▁તેમ ▁જ ▁દૂધ ની</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_lm.show_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "250d8059",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lm.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d6ce3401",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = language_model_learner(data_lm, TransformerXL, pretrained=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "19e96b4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='0' class='' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/1 00:00<00:00]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='89' class='' max='12592' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.71% [89/12592 02:30<5:52:41 25.1083]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jazzyy/anaconda3/lib/python3.9/site-packages/fastai/text/models/transformer.py:114: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at  /opt/conda/conda-bld/pytorch_1646755849709/work/aten/src/ATen/native/TensorAdvancedIndexing.cpp:1391.)\n",
      "  attn_score = attn_score.float().masked_fill(mask, -float('inf')).type_as(attn_score)\n",
      "/home/jazzyy/anaconda3/lib/python3.9/site-packages/torch/autograd/__init__.py:173: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at  /opt/conda/conda-bld/pytorch_1646755849709/work/aten/src/ATen/native/TensorAdvancedIndexing.cpp:1391.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n"
     ]
    }
   ],
   "source": [
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b7a3cf58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "23272ddd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequentialRNN(\n",
       "  (0): TransformerXL(\n",
       "    (encoder): Embedding(20000, 410)\n",
       "    (pos_enc): PositionalEncoding()\n",
       "    (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0): DecoderLayer(\n",
       "        (mhra): MultiHeadRelativeAttention(\n",
       "          (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "          (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "          (drop_att): Dropout(p=0.1, inplace=False)\n",
       "          (drop_res): Dropout(p=0.1, inplace=False)\n",
       "          (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "          (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Dropout(p=0.1, inplace=False)\n",
       "            (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "            (4): Dropout(p=0.1, inplace=False)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): DecoderLayer(\n",
       "        (mhra): MultiHeadRelativeAttention(\n",
       "          (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "          (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "          (drop_att): Dropout(p=0.1, inplace=False)\n",
       "          (drop_res): Dropout(p=0.1, inplace=False)\n",
       "          (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "          (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Dropout(p=0.1, inplace=False)\n",
       "            (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "            (4): Dropout(p=0.1, inplace=False)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): DecoderLayer(\n",
       "        (mhra): MultiHeadRelativeAttention(\n",
       "          (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "          (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "          (drop_att): Dropout(p=0.1, inplace=False)\n",
       "          (drop_res): Dropout(p=0.1, inplace=False)\n",
       "          (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "          (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Dropout(p=0.1, inplace=False)\n",
       "            (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "            (4): Dropout(p=0.1, inplace=False)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): DecoderLayer(\n",
       "        (mhra): MultiHeadRelativeAttention(\n",
       "          (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "          (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "          (drop_att): Dropout(p=0.1, inplace=False)\n",
       "          (drop_res): Dropout(p=0.1, inplace=False)\n",
       "          (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "          (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Dropout(p=0.1, inplace=False)\n",
       "            (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "            (4): Dropout(p=0.1, inplace=False)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): DecoderLayer(\n",
       "        (mhra): MultiHeadRelativeAttention(\n",
       "          (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "          (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "          (drop_att): Dropout(p=0.1, inplace=False)\n",
       "          (drop_res): Dropout(p=0.1, inplace=False)\n",
       "          (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "          (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Dropout(p=0.1, inplace=False)\n",
       "            (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "            (4): Dropout(p=0.1, inplace=False)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): DecoderLayer(\n",
       "        (mhra): MultiHeadRelativeAttention(\n",
       "          (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "          (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "          (drop_att): Dropout(p=0.1, inplace=False)\n",
       "          (drop_res): Dropout(p=0.1, inplace=False)\n",
       "          (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "          (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Dropout(p=0.1, inplace=False)\n",
       "            (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "            (4): Dropout(p=0.1, inplace=False)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (6): DecoderLayer(\n",
       "        (mhra): MultiHeadRelativeAttention(\n",
       "          (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "          (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "          (drop_att): Dropout(p=0.1, inplace=False)\n",
       "          (drop_res): Dropout(p=0.1, inplace=False)\n",
       "          (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "          (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Dropout(p=0.1, inplace=False)\n",
       "            (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "            (4): Dropout(p=0.1, inplace=False)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (7): DecoderLayer(\n",
       "        (mhra): MultiHeadRelativeAttention(\n",
       "          (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "          (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "          (drop_att): Dropout(p=0.1, inplace=False)\n",
       "          (drop_res): Dropout(p=0.1, inplace=False)\n",
       "          (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "          (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Dropout(p=0.1, inplace=False)\n",
       "            (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "            (4): Dropout(p=0.1, inplace=False)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (8): DecoderLayer(\n",
       "        (mhra): MultiHeadRelativeAttention(\n",
       "          (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "          (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "          (drop_att): Dropout(p=0.1, inplace=False)\n",
       "          (drop_res): Dropout(p=0.1, inplace=False)\n",
       "          (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "          (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Dropout(p=0.1, inplace=False)\n",
       "            (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "            (4): Dropout(p=0.1, inplace=False)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (9): DecoderLayer(\n",
       "        (mhra): MultiHeadRelativeAttention(\n",
       "          (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "          (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "          (drop_att): Dropout(p=0.1, inplace=False)\n",
       "          (drop_res): Dropout(p=0.1, inplace=False)\n",
       "          (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "          (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Dropout(p=0.1, inplace=False)\n",
       "            (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "            (4): Dropout(p=0.1, inplace=False)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (10): DecoderLayer(\n",
       "        (mhra): MultiHeadRelativeAttention(\n",
       "          (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "          (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "          (drop_att): Dropout(p=0.1, inplace=False)\n",
       "          (drop_res): Dropout(p=0.1, inplace=False)\n",
       "          (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "          (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Dropout(p=0.1, inplace=False)\n",
       "            (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "            (4): Dropout(p=0.1, inplace=False)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (11): DecoderLayer(\n",
       "        (mhra): MultiHeadRelativeAttention(\n",
       "          (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "          (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "          (drop_att): Dropout(p=0.1, inplace=False)\n",
       "          (drop_res): Dropout(p=0.1, inplace=False)\n",
       "          (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "          (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "        )\n",
       "        (ff): SequentialEx(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Dropout(p=0.1, inplace=False)\n",
       "            (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "            (4): Dropout(p=0.1, inplace=False)\n",
       "            (5): MergeLayer()\n",
       "            (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (1): LinearDecoder(\n",
       "    (decoder): Linear(in_features=410, out_features=20000, bias=True)\n",
       "    (output_dp): RNNDropout()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefb35d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='1' class='' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      5.00% [1/20 6:16:08<119:06:43]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>4.757486</td>\n",
       "      <td>5.065322</td>\n",
       "      <td>0.322035</td>\n",
       "      <td>6:16:04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='4460' class='' max='12592' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      35.42% [4460/12592 1:53:33<3:27:03 4.9208]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 0 with accuracy value: 0.3220352232456207.\n"
     ]
    }
   ],
   "source": [
    "learn.fit_one_cycle(20, 1e-3, moms=(0.8,0.7), callbacks=[callbacks.SaveModelCallback(learn, every='improvement', monitor='accuracy', name='model')])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "686be9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path('/home/jazzyy/Desktop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "267ea225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "઻૏ઢ૊ ઺઺ -0.053127 0.092212 0.052078 -0.47957 -0.15389 0.012342 -0.024682 -0.086522 -0.32376 0.15922 0.25405 -0.16875 0.15952 0.027825 0.36839 0.092198 0.034502 0.25046 -0.17594 -0.088164 -0.3114 -0.16498 -0.080611 -0.016165 0.12734 0.03436 -0.10277 -0.0066375 0.099743 -0.049142 -0.22037 -0.22111 -0.13935 -0.10155 -0.047634 -0.10184 -0.073968 -0.17888 0.18667 0.07063 0.12148 -0.18779 -0.030857 0.15846 -0.11462 -0.14845 0.27495 0.11392 0.32192 0.13084 -0.17301 0.09571 0.074519 0.20547 -0.13818 0.10479 0.052833 0.070008 -0.029147 -0.011447 0.10226 0.52079 0.10353 0.043352 -0.093923 0.086996 0.053583 -0.18585 -0.00037937 0.023372 -0.023339 0.36972 0.045623 0.047414 -0.029451 -0.0085147 0.038774 -0.094087 0.34805 0.22199 0.14746 -0.093993 0.043505 0.065589 -0.12813 0.19216 0.012124 0.19022 0.039524 0.087403 -0.15943 -0.1061 -0.13629 0.11055 0.084009 0.23109 0.066964 0.36336 -0.013235 -0.046794 -0.14371 -0.19505 -0.33072 0.24404 0.0059492 -0.26283 -0.16015 -0.021731 -0.057479 -0.12912 0.018372 -0.1721 -0.39867 -0.071515 -0.29968 -0.36566 0.16974 0.037114 -0.22213 -0.27146 0.25892 -0.13366 0.072986 0.39836 -0.3062 -0.4537 -0.0023554 0.082935 0.13929 -0.017761 -0.28143 0.15271 0.25331 0.062852 -0.085707 0.10734 -0.081029 0.011679 0.12368 0.1191 -0.34354 -0.33426 -0.091555 -0.30164 0.084946 -0.26453 0.10844 -0.12112 0.049708 0.17414 0.26308 -0.063693 0.13707 -0.10547 -0.029695 -0.31275 0.047644 -0.025316 0.042093 -0.10443 -0.062396 -0.017544 -0.047038 -0.15992 -0.19679 -0.031472 0.22725 -0.096616 -0.042949 -0.07505 0.025542 0.10027 -0.33034 -0.0413 0.077712 -0.0039111 -0.099301 0.12017 -0.2454 -0.03116 0.30661 -0.0012165 0.19051 -0.058913 -0.11661 -0.17893 -0.22365 0.16978 0.076809 0.053969 0.01289 0.0085428 -0.093586 0.30741 0.075548 -0.1001 -0.25976 -0.12606 -0.045049 -0.18956 0.18362 -0.081316 0.012964 -0.22466 0.21003 0.22674 0.28712 -0.14509 0.14165 -0.0087761 0.26388 -0.068933 -0.053056 0.048934 -0.15164 -0.12287 0.052747 0.33887 -0.13412 0.30975 0.20664 -0.081042 -0.56739 -0.075733 0.014444 -0.078615 0.24105 -0.16291 -0.15559 0.014233 -0.10504 0.083541 -0.029405 -0.16566 -0.24146 0.035941 0.064932 0.1128 0.13735 -0.25394 0.035775 0.12293 0.15863 0.096227 0.025112 0.33087 0.17259 0.60302 -0.045432 0.024909 -0.3302 -0.088594 -0.33054 0.14389 -0.11271 0.040652 0.13115 0.064394 -0.11705 -0.094208 0.079758 -0.024017 0.17223 0.052058 -0.03745 -0.15572 -0.15597 0.038632 0.30614 0.56944 0.41286 -0.2708 -0.25572 0.093871 -0.21393 -0.20535 0.22456 0.30541 0.016898 0.40819 -0.10556 0.074022 0.25533 -0.59263 0.20328 0.39502 0.31503 -0.18351 -0.77936 -0.024739 0.29539 -0.34447 -0.15419 0.008072 0.48989 -0.19699 -0.059518 -0.13545 -0.050621 -0.040352 \n",
      "\n",
      "Found 470297 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "f = open(os.path.join(path, 'indicnlp.v1.gu.txt'))\n",
    "for line in f:\n",
    "    try:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    except:\n",
    "        print(line)\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "0882637a",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = Tokenizer().word_index\n",
    "EMBEDDING_DIM = 100\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "3cefc814",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "8752d151",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len_text=80 \n",
    "max_len_summary=10\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_tr,x_val,y_tr,y_val=train_test_split('/home/jazzyy/Text_Summ/language-model/train/','/home/jazzyy/Text_Summ/language-model/valid/',test_size=0.1,random_state=0,shuffle=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "961a6157",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tokenizer = Tokenizer()\n",
    "x_tokenizer.fit_on_texts(list(x_tr))\n",
    "\n",
    "#convert text sequences into integer sequences\n",
    "x_tr    =   x_tokenizer.texts_to_sequences(x_tr) \n",
    "x_val   =   x_tokenizer.texts_to_sequences(x_val)\n",
    "\n",
    "#padding zero upto maximum length\n",
    "x_tr    =   pad_sequences(x_tr,  maxlen=max_len_text, padding='post') \n",
    "x_val   =   pad_sequences(x_val, maxlen=max_len_text, padding='post')\n",
    "\n",
    "x_voc_size   =  len(x_tokenizer.word_index) +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "d6675dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "from tensorflow.python.keras.layers import Layer\n",
    "from tensorflow.python.keras import backend as K\n",
    "\n",
    "\n",
    "class AttentionLayer(Layer):\n",
    "    \"\"\"\n",
    "    This class implements Bahdanau attention (https://arxiv.org/pdf/1409.0473.pdf).\n",
    "    There are three sets of weights introduced W_a, U_a, and V_a\n",
    "     \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert isinstance(input_shape, list)\n",
    "        # Create a trainable weight variable for this layer.\n",
    "\n",
    "        self.W_a = self.add_weight(name='W_a',\n",
    "                                   shape=tf.TensorShape((input_shape[0][2], input_shape[0][2])),\n",
    "                                   initializer='uniform',\n",
    "                                   trainable=True)\n",
    "        self.U_a = self.add_weight(name='U_a',\n",
    "                                   shape=tf.TensorShape((input_shape[1][2], input_shape[0][2])),\n",
    "                                   initializer='uniform',\n",
    "                                   trainable=True)\n",
    "        self.V_a = self.add_weight(name='V_a',\n",
    "                                   shape=tf.TensorShape((input_shape[0][2], 1)),\n",
    "                                   initializer='uniform',\n",
    "                                   trainable=True)\n",
    "\n",
    "        super(AttentionLayer, self).build(input_shape)  # Be sure to call this at the end\n",
    "\n",
    "    def call(self, inputs, verbose=False):\n",
    "        \"\"\"\n",
    "        inputs: [encoder_output_sequence, decoder_output_sequence]\n",
    "        \"\"\"\n",
    "        assert type(inputs) == list\n",
    "        encoder_out_seq, decoder_out_seq = inputs\n",
    "        if verbose:\n",
    "            print('encoder_out_seq>', encoder_out_seq.shape)\n",
    "            print('decoder_out_seq>', decoder_out_seq.shape)\n",
    "\n",
    "        def energy_step(inputs, states):\n",
    "            \"\"\" Step function for computing energy for a single decoder state \"\"\"\n",
    "\n",
    "            assert_msg = \"States must be a list. However states {} is of type {}\".format(states, type(states))\n",
    "            assert isinstance(states, list) or isinstance(states, tuple), assert_msg\n",
    "\n",
    "            \"\"\" Some parameters required for shaping tensors\"\"\"\n",
    "            en_seq_len, en_hidden = encoder_out_seq.shape[1], encoder_out_seq.shape[2]\n",
    "            de_hidden = inputs.shape[-1]\n",
    "\n",
    "            \"\"\" Computing S.Wa where S=[s0, s1, ..., si]\"\"\"\n",
    "            # <= batch_size*en_seq_len, latent_dim\n",
    "            reshaped_enc_outputs = K.reshape(encoder_out_seq, (-1, en_hidden))\n",
    "            # <= batch_size*en_seq_len, latent_dim\n",
    "            W_a_dot_s = K.reshape(K.dot(reshaped_enc_outputs, self.W_a), (-1, en_seq_len, en_hidden))\n",
    "            if verbose:\n",
    "                print('wa.s>',W_a_dot_s.shape)\n",
    "\n",
    "            \"\"\" Computing hj.Ua \"\"\"\n",
    "            U_a_dot_h = K.expand_dims(K.dot(inputs, self.U_a), 1)  # <= batch_size, 1, latent_dim\n",
    "            if verbose:\n",
    "                print('Ua.h>',U_a_dot_h.shape)\n",
    "\n",
    "            \"\"\" tanh(S.Wa + hj.Ua) \"\"\"\n",
    "            # <= batch_size*en_seq_len, latent_dim\n",
    "            reshaped_Ws_plus_Uh = K.tanh(K.reshape(W_a_dot_s + U_a_dot_h, (-1, en_hidden)))\n",
    "            if verbose:\n",
    "                print('Ws+Uh>', reshaped_Ws_plus_Uh.shape)\n",
    "\n",
    "            \"\"\" softmax(va.tanh(S.Wa + hj.Ua)) \"\"\"\n",
    "            # <= batch_size, en_seq_len\n",
    "            e_i = K.reshape(K.dot(reshaped_Ws_plus_Uh, self.V_a), (-1, en_seq_len))\n",
    "            # <= batch_size, en_seq_len\n",
    "            e_i = K.softmax(e_i)\n",
    "\n",
    "            if verbose:\n",
    "                print('ei>', e_i.shape)\n",
    "\n",
    "            return e_i, [e_i]\n",
    "\n",
    "        def context_step(inputs, states):\n",
    "            \"\"\" Step function for computing ci using ei \"\"\"\n",
    "            # <= batch_size, hidden_size\n",
    "            c_i = K.sum(encoder_out_seq * K.expand_dims(inputs, -1), axis=1)\n",
    "            if verbose:\n",
    "                print('ci>', c_i.shape)\n",
    "            return c_i, [c_i]\n",
    "\n",
    "        def create_inital_state(inputs, hidden_size):\n",
    "            # We are not using initial states, but need to pass something to K.rnn funciton\n",
    "            fake_state = K.zeros_like(inputs)  # <= (batch_size, enc_seq_len, latent_dim\n",
    "            fake_state = K.sum(fake_state, axis=[1, 2])  # <= (batch_size)\n",
    "            fake_state = K.expand_dims(fake_state)  # <= (batch_size, 1)\n",
    "            fake_state = K.tile(fake_state, [1, hidden_size])  # <= (batch_size, latent_dim\n",
    "            return fake_state\n",
    "\n",
    "        fake_state_c = create_inital_state(encoder_out_seq, encoder_out_seq.shape[-1])\n",
    "        fake_state_e = create_inital_state(encoder_out_seq, encoder_out_seq.shape[1])  # <= (batch_size, enc_seq_len, latent_dim\n",
    "\n",
    "        \"\"\" Computing energy outputs \"\"\"\n",
    "        # e_outputs => (batch_size, de_seq_len, en_seq_len)\n",
    "        last_out, e_outputs, _ = K.rnn(\n",
    "            energy_step, decoder_out_seq, [fake_state_e],\n",
    "        )\n",
    "\n",
    "        \"\"\" Computing context vectors \"\"\"\n",
    "        last_out, c_outputs, _ = K.rnn(\n",
    "            context_step, e_outputs, [fake_state_c],\n",
    "        )\n",
    "\n",
    "        return c_outputs, e_outputs\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        \"\"\" Outputs produced by the layer \"\"\"\n",
    "        return [\n",
    "            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[1][2])),\n",
    "            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[0][1]))\n",
    "        ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "77e076ab",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Layer embedding weight shape (19, 100) is not compatible with provided weight shape (1, 100).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_18384/4087244905.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Encoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mencoder_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_len_text\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0menc_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_voc_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0membedding_matrix\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_len_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m#LSTM 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36mset_weights\u001b[0;34m(self, weights)\u001b[0m\n\u001b[1;32m   1862\u001b[0m         \u001b[0mref_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1863\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mref_shape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1864\u001b[0;31m           raise ValueError(\n\u001b[0m\u001b[1;32m   1865\u001b[0m               \u001b[0;34mf'Layer {self.name} weight shape {ref_shape} '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1866\u001b[0m               \u001b[0;34m'is not compatible with provided weight '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Layer embedding weight shape (19, 100) is not compatible with provided weight shape (1, 100)."
     ]
    }
   ],
   "source": [
    "from keras import backend as K \n",
    "K.clear_session() \n",
    "latent_dim = 500 \n",
    "\n",
    "# Encoder \n",
    "encoder_inputs = Input(shape=(max_len_text,)) \n",
    "enc_emb = Embedding(x_voc_size, 100, weights=[embedding_matrix],input_length=max_len_text, trainable=False)(encoder_inputs) \n",
    "\n",
    "#LSTM 1 \n",
    "encoder_lstm1 = LSTM(latent_dim,return_sequences=True,return_state=True) \n",
    "encoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb) \n",
    "\n",
    "#LSTM 2 \n",
    "encoder_lstm2 = LSTM(latent_dim,return_sequences=True,return_state=True) \n",
    "encoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1) \n",
    "\n",
    "#LSTM 3 \n",
    "encoder_lstm3=LSTM(latent_dim, return_state=True, return_sequences=True) \n",
    "encoder_outputs, state_h, state_c= encoder_lstm3(encoder_output2) \n",
    "\n",
    "# Set up the decoder. \n",
    "decoder_inputs = Input(shape=(None,)) \n",
    "dec_emb_layer = Embedding(x_voc_size, 100, weights=[embedding_matrix],input_length=max_len_text, trainable=False) \n",
    "\n",
    "dec_emb = dec_emb_layer(decoder_inputs) \n",
    "\n",
    "#LSTM using encoder_states as initial state\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True) \n",
    "decoder_outputs,decoder_fwd_state, decoder_back_state = decoder_lstm(dec_emb,initial_state=[state_h, state_c]) \n",
    "\n",
    "#Attention Layer\n",
    "attn_layer = AttentionLayer(name='attention_layer') \n",
    "attn_out, attn_states = attn_layer([encoder_outputs, decoder_outputs]) \n",
    "\n",
    "# Concat attention output and decoder LSTM output \n",
    "decoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_outputs, attn_out])\n",
    "\n",
    "#Dense layer\n",
    "decoder_dense = TimeDistributed(Dense(y_voc_size, activation='softmax')) \n",
    "decoder_outputs = decoder_dense(decoder_concat_input) \n",
    "\n",
    "# Define the model\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs) \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72657dc6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
