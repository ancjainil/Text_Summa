{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b0bd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load preprocessor.py\n",
    "{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 1,\n",
    "   \"id\": \"ee4b9d4a\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"ename\": \"ImportError\",\n",
    "     \"evalue\": \"cannot import name 'WordTokenizer' from 'tokenizer' (/home/jazzyy/anaconda3/lib/python3.9/site-packages/tokenizer/__init__.py)\",\n",
    "     \"output_type\": \"error\",\n",
    "     \"traceback\": [\n",
    "      \"\\u001b[0;31m---------------------------------------------------------------------------\\u001b[0m\",\n",
    "      \"\\u001b[0;31mImportError\\u001b[0m                               Traceback (most recent call last)\",\n",
    "      \"\\u001b[0;32m/tmp/ipykernel_41293/1700810879.py\\u001b[0m in \\u001b[0;36m<module>\\u001b[0;34m\\u001b[0m\\n\\u001b[1;32m      1\\u001b[0m \\u001b[0;32mimport\\u001b[0m \\u001b[0mre\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[0;32m----> 2\\u001b[0;31m \\u001b[0;32mfrom\\u001b[0m \\u001b[0mtokenizer\\u001b[0m \\u001b[0;32mimport\\u001b[0m \\u001b[0mWordTokenizer\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[0m\\u001b[1;32m      3\\u001b[0m \\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[1;32m      4\\u001b[0m \\u001b[0;32mclass\\u001b[0m \\u001b[0mPreprocessor\\u001b[0m\\u001b[0;34m(\\u001b[0m\\u001b[0;34m)\\u001b[0m\\u001b[0;34m:\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\\u001b[1;32m      5\\u001b[0m     \\u001b[0;32mdef\\u001b[0m \\u001b[0m__init__\\u001b[0m\\u001b[0;34m(\\u001b[0m\\u001b[0mself\\u001b[0m\\u001b[0;34m)\\u001b[0m\\u001b[0;34m:\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0;34m\\u001b[0m\\u001b[0m\\n\",\n",
    "      \"\\u001b[0;31mImportError\\u001b[0m: cannot import name 'WordTokenizer' from 'tokenizer' (/home/jazzyy/anaconda3/lib/python3.9/site-packages/tokenizer/__init__.py)\"\n",
    "     ]\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"import re\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"class Preprocessor():\\n\",\n",
    "    \"    def __init__(self):\\n\",\n",
    "    \"        self.suffixes = []\\n\",\n",
    "    \"        pass\\n\",\n",
    "    \"\\n\",\n",
    "    \"    def compulsory_preprocessing(self, text):\\n\",\n",
    "    \"        '''This is a function to preprocess the text and make the necessary changes which are compulsory for any type of Gujarati NLP task'''\\n\",\n",
    "    \"        text = re.sub(r'\\\\u200b', '', text)\\n\",\n",
    "    \"        text = re.sub(r'\\\\ufeff', \\\"\\\", text)\\n\",\n",
    "    \"        text = re.sub(r'…', \\\" \\\", text)\\n\",\n",
    "    \"        text = re.sub(r'  ', ' ', text)\\n\",\n",
    "    \"        text = re.sub(r'”“', '', text)\\n\",\n",
    "    \"        text = WordTokenizer(text)\\n\",\n",
    "    \"        for i in range(len(text)):\\n\",\n",
    "    \"            text[i] = text[i].rstrip(':')\\n\",\n",
    "    \"        return ' '.join(text)\\n\",\n",
    "    \"\\n\",\n",
    "    \"    def remove_tek(self, text, tek_string):\\n\",\n",
    "    \"        '''\\n\",\n",
    "    \"        Tek is the Gujarati word for the initial line of the poem. Whenever, one stanza of any poem is sung, the initial line of the poem is sung once again before starting the\\n\",\n",
    "    \"        next stanza. This is called as singing a \\\"Tek\\\". Written poems mention the tek string too many a times. This will cause a problem of redundancy. Hence, removing it is\\n\",\n",
    "    \"        necessary.\\n\",\n",
    "    \"        '''\\n\",\n",
    "    \"        if str(type(tek_string))==\\\"<class 'NoneType'>\\\" or not tek_string:\\n\",\n",
    "    \"            raise TypeError('tek_string needs to be a valid string')\\n\",\n",
    "    \"        if str(type(text))==\\\"<class 'list'>\\\":\\n\",\n",
    "    \"            for i in range(len(text)):\\n\",\n",
    "    \"                text[i] = text[i].rstrip(tek_string)\\n\",\n",
    "    \"        elif str(type(text))==\\\"<class 'str'>\\\":\\n\",\n",
    "    \"            text = text.rstrip(tek_string)\\n\",\n",
    "    \"        else:\\n\",\n",
    "    \"            raise TypeError(\\\"Argument 'text' must be either a str or list\\\")\\n\",\n",
    "    \"        return text\\n\",\n",
    "    \"\\n\",\n",
    "    \"    def poetic_preprocessing(self, text, remove_tek=False, tek_string=None):\\n\",\n",
    "    \"        '''This function is only required when dealing with poetic corpora. Make sure to use this function along with the compulsory preprocessing to have decently accurate results with poetic corpora'''\\n\",\n",
    "    \"        text = re.sub(r'।','.',text)\\n\",\n",
    "    \"        text = re.sub(' ।।[૧૨૩૪૫૬૭૮૯૦]।।', '.', text)\\n\",\n",
    "    \"        if remove_tek:\\n\",\n",
    "    \"            text = self.remove_tek(text, tek_string)\\n\",\n",
    "    \"        tokens = WordTokenizer(text, corpus='poetry', keep_punctuations=False)\\n\",\n",
    "    \"\\n\",\n",
    "    \"        for i in range(len(tokens)):\\n\",\n",
    "    \"            # Rule 1\\n\",\n",
    "    \"            if tokens[i].endswith('જી'):\\n\",\n",
    "    \"                tokens[i] = tokens[i].strip('જી')\\n\",\n",
    "    \"            # Rule 2\\n\",\n",
    "    \"            if tokens[i].endswith('ૈ'):\\n\",\n",
    "    \"                tokens[i] = tokens[i].strip('ૈ')+'ે'\\n\",\n",
    "    \"            # Rule 3\\n\",\n",
    "    \"            index = tokens[i].find('ર')\\n\",\n",
    "    \"            if index == -1:\\n\",\n",
    "    \"                pass\\n\",\n",
    "    \"            elif index<len(tokens[i])-1 and tokens[i][index-1]=='િ':\\n\",\n",
    "    \"                tokens[i] = re.sub('િર', 'ૃ', tokens[i])\\n\",\n",
    "    \"\\n\",\n",
    "    \"        return ' '.join(tokens)\"\n",
    "   ]\n",
    "  }\n",
    "  \n",
    " ],\n",
    " \"metadata\": {\n",
    "  \"kernelspec\": {\n",
    "   \"display_name\": \"Python 3 (ipykernel)\",\n",
    "   \"language\": \"python\",\n",
    "   \"name\": \"python3\"\n",
    "  },\n",
    "  \"language_info\": {\n",
    "   \"codemirror_mode\": {\n",
    "    \"name\": \"ipython\",\n",
    "    \"version\": 3\n",
    "   },\n",
    "   \"file_extension\": \".py\",\n",
    "   \"mimetype\": \"text/x-python\",\n",
    "   \"name\": \"python\",\n",
    "   \"nbconvert_exporter\": \"python\",\n",
    "   \"pygments_lexer\": \"ipython3\",\n",
    "   \"version\": \"3.9.7\"\n",
    "  }\n",
    " },\n",
    " \"nbformat\": 4,\n",
    " \"nbformat_minor\": 5\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d9af2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "suffixes = ['નાં','ના','ની','નો','નું','ને','થી','માં','એ','ઓ','ે','તા','તી','વા','મા','વું','વુ','ો','માંથી','શો','ીશ','ીશું','શે',\n",
    "'તો','તું','તાં','્યો','યો','યાં','્યું','યું','્યા','યા','્યાં','સ્વી','રે','ં','મ્','મ્','ી','કો']\n",
    "prefixes = ['અ']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c0663d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from utils.StopWords import *\n",
    "import preprocessor as p\n",
    "\n",
    "import re\n",
    "\n",
    "# You may need to add/remove suffixes/prefixes according to the corpora\n",
    "suffixes = ['નાં','ના','ની','નો','નું','ને','થી','માં','એ','ઓ','ે','તા','તી','વા','મા','વું','વુ','ો','માંથી','શો','ીશ','ીશું','શે',\n",
    "    'તો','તું','તાં','્યો','યો','યાં','્યું','યું','્યા','યા','્યાં','સ્વી','રે','ં','મ્','મ્','ી','કો']\n",
    "prefixes = ['અ']\n",
    "\n",
    "\n",
    "\n",
    "class Stemmer():\n",
    "    def __init__(self):\n",
    "        self.suffixes = suffixes\n",
    "        self.prefixes = prefixes\n",
    "\n",
    "        self.suffixes.append(suffix)\n",
    "\n",
    "    def add_prefix(self, prefix):\n",
    "        self.prefixes.append(prefix)\n",
    "\n",
    "    def delete_suffix(self, suffix):\n",
    "        try:\n",
    "            del(self.suffixes[self.suffixes.index(suffix)])\n",
    "        except IndexError:\n",
    "            print('{} not present in suffixes'.format(suffix))\n",
    "\n",
    "    def delete_prefix(self, prefix):\n",
    "        try:\n",
    "            del(self.prefixes[self.prefixes.index(prefix)])\n",
    "        except IndexError:\n",
    "            print(\"{} not present in prefixes\".format(prefix))\n",
    "\n",
    "\n",
    "    def stem_word(self, sentence, corpus):\n",
    "        word_list = sentence.strip('\\u200b').split(' ')\n",
    "        if not word_list[-1]:\n",
    "            del(word_list[-1])\n",
    "        return_list = []\n",
    "        puctuations = ('.',',','!','?','\"',\"'\",'%','#','@','&','…')\n",
    "        for word in word_list:\n",
    "            a = word\n",
    "            if word.endswith(puctuations):\n",
    "                a = word[:-1]\n",
    "            if corpus == 'prose':\n",
    "                if a in prose_stopwords:\n",
    "                    return_list.append(a)\n",
    "                    continue\n",
    "            else:\n",
    "                if a in poetry_stopwords:\n",
    "                    return_list.append(a)\n",
    "                    continue\n",
    "            for suffix in suffixes:\n",
    "                if a.endswith(suffix):\n",
    "                    a = a.rstrip(suffix)\n",
    "                    break\n",
    "            for prefix in prefixes:\n",
    "                if a.startswith(prefix):\n",
    "                    a = a.lstrip(prefix)\n",
    "                    break\n",
    "            if word.endswith(puctuations):\n",
    "                a+=str(word[-1])\n",
    "            return_list.append(a)\n",
    "        return_sentence = \" \".join(return_list)\n",
    "        return return_sentence\n",
    "\n",
    "    def stem(self, text, corpus='prose', remove_tek=False, tek_string=None):\n",
    "        preprocessor = p()\n",
    "        text = preprocessor.compulsory_preprocessing(text)\n",
    "        if corpus == 'poetry':\n",
    "            text = preprocessor.poetic_preprocessing(text, remove_tek=remove_tek, tek_string=tek_string)\n",
    "        elif corpus == 'prose':\n",
    "            pass\n",
    "        else:\n",
    "            raise ValueError(\"Unnrecognized argument 'corpus'. Should be either 'prose' or 'poetry'\")\n",
    "        l = SentenceTokenizer(text)\n",
    "        if len(l)==1:\n",
    "            sentence = l[0]\n",
    "            return self.stem_word(sentence, corpus=corpus)\n",
    "        else:\n",
    "            a = []\n",
    "            for sentence in l:\n",
    "                a.append(self.stem(sentence))\n",
    "            return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0b74fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'હું દોડ​વા જઉં છું. દોડ​વા માટે તમારે સાથે આવ​વું પ‌ડશે. ચાલશે ને? અન્યાય ના કરતા.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2aed7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prepro = "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
